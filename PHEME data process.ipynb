{"cells":[{"cell_type":"code","execution_count":null,"id":"7fca3ed7","metadata":{"id":"7fca3ed7"},"outputs":[],"source":["import numpy as np\n","import os\n","import jsonlines\n","import numpy as np\n","import torch"]},{"cell_type":"code","execution_count":null,"id":"f93c5d5f","metadata":{"id":"f93c5d5f"},"outputs":[],"source":["# What need to be collect\n","\n","# for each single event: \n","# 1. source nodes\n","# 2. destination nodes\n","# 3. time_stamps\n","# 4. raw_node_features\n","# 5. edge_idxs\n","# *: Neighbor_finder, need time_span. \n","\n","'''\n","Time span should be the interval from the source post to current. Therefore,\n","it's equivalent to my collected time_stamps. \n","\n","And you will need a pretrained BERT to collect raw_node_features\n","'''"]},{"cell_type":"code","execution_count":null,"id":"411d5e32","metadata":{"id":"411d5e32","outputId":"b73b5d50-2583-4d68-b0aa-98145472e999"},"outputs":[{"name":"stdout","output_type":"stream","text":["length of traininig data: 4641\n","length of devolop data:  580\n","length of test data:  581\n","length of whole data:  5802\n"]}],"source":["# Load training data as train_dataï¼Œ write into train_file, test_file and label_file\n","# Load PHEME dataset. \n","\n","\n","path = 'project-data/'\n","train_data_path = 'train.data.jsonl'\n","dev_data_path = 'dev.data.jsonl'\n","test_data_path = 'test.data.jsonl'\n","\n","train_data = []\n","with jsonlines.open(path + train_data_path) as reader:\n","    for obj in reader:\n","        train_data.append(obj)\n","print('length of traininig data:', len(train_data))\n","\n","        \n","# load the development data as dev_data(used as test_data in RvNN)\n","dev_data = []\n","with jsonlines.open(path + dev_data_path) as reader1:\n","    for obj in reader1:\n","        dev_data.append(obj)\n","print('length of devolop data: ', len(dev_data))\n","\n","\n","test_data = []\n","with jsonlines.open(path + test_data_path) as reader2:\n","    for obj in reader2:\n","        test_data.append(obj)\n","print('length of test data: ', len(test_data))\n","\n","raw_PHEME = train_data + dev_data + test_data\n","print('length of whole data: ', len(raw_PHEME))\n","\n","import json\n","\n","label_path = 'PHEME_label.json'\n","with open(path+label_path) as f:\n","    PHEME_label = json.load(f)\n","\n","\n"]},{"cell_type":"code","execution_count":null,"id":"3b7252d9","metadata":{"id":"3b7252d9"},"outputs":[],"source":["def time_stamp(dic):\n","    # given a tweet dict\n","    month_list = ['Jan', 'Feb', 'Mar', 'Apr', 'May', 'Jun', 'Jul', \"Aug\", 'Sep', 'Oct', 'Nov', 'Dec']\n","    time = dic['created_at']\n","    month = time[4:7]\n","    day = time[8:10]\n","    h = time[11:13]\n","    m = time[14:16]\n","    s = time[17:19]\n","    return month_list.index(month)*86400*30 + int(day)*86400 + int(h)*3600 + int(m)*60 + int(s)*1\n","\n","'''\n","for idx in range(len(train_data)):\n","    train_data[idx] = sorted(train_data[idx], key=lambda tree_node: time_stamp(tree_node))\n","\n","for idx in range(len(test_data)):\n","    test_data[idx] = sorted(test_data[idx], key=lambda tree_node: time_stamp(tree_node))\n","\n","for idx in range(len(dev_data)):\n","    dev_data[idx] = sorted(dev_data[idx], key=lambda tree_node: time_stamp(tree_node))\n","'''\n","for idx in range(len(raw_PHEME)):\n","    raw_PHEME[idx] = sorted(raw_PHEME[idx], key=lambda tree_node: time_stamp(tree_node))"]},{"cell_type":"code","execution_count":null,"id":"d06cdb14","metadata":{"id":"d06cdb14"},"outputs":[],"source":["dataset = {}\n","for item in raw_PHEME:\n","    ID = item[0]['id_str']\n","    dataset[ID] = {}\n","    \n","    "]},{"cell_type":"code","execution_count":null,"id":"79c95d3c","metadata":{"id":"79c95d3c","outputId":"016778b4-562c-459f-8afb-305c70208f97"},"outputs":[{"name":"stdout","output_type":"stream","text":["dict_keys(['contributors', 'truncated', 'text', 'in_reply_to_status_id', 'id', 'favorite_count', 'source', 'retweeted', 'coordinates', 'entities', 'in_reply_to_screen_name', 'id_str', 'retweet_count', 'in_reply_to_user_id', 'favorited', 'user', 'geo', 'in_reply_to_user_id_str', 'possibly_sensitive', 'lang', 'created_at', 'in_reply_to_status_id_str', 'place', 'extended_entities'])\n","dict_keys(['contributors', 'truncated', 'text', 'in_reply_to_status_id', 'id', 'favorite_count', 'source', 'retweeted', 'coordinates', 'entities', 'in_reply_to_screen_name', 'id_str', 'retweet_count', 'in_reply_to_user_id', 'favorited', 'user', 'geo', 'in_reply_to_user_id_str', 'lang', 'created_at', 'in_reply_to_status_id_str', 'place'])\n"]}],"source":["print(raw_PHEME[0][0].keys())\n","print(raw_PHEME[0][1].keys())"]},{"cell_type":"code","execution_count":null,"id":"0f4352b4","metadata":{"id":"0f4352b4","outputId":"000cab55-432a-4eae-cc78-12775bd41fac"},"outputs":[{"name":"stderr","output_type":"stream","text":["Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.weight']\n","- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"]}],"source":["from transformers import BertModel\n","from transformers import BertTokenizer\n","bert_model = BertModel.from_pretrained('bert-base-uncased')\n","tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n","\n","def bert_module(sentence):\n","    \n","    tokens = tokenizer.tokenize(sentence)\n","    tokens = ['[CLS]'] + tokens + ['[SEP]']\n","    attn_mask = [1 for _ in range(len(tokens))]\n","    seg_ids = [0 for _ in range(len(tokens))]\n","    token_ids = tokenizer.convert_tokens_to_ids(tokens)\n","    \n","    token_ids_tensor = torch.tensor(token_ids).unsqueeze(dim=0)\n","    attn_mask_tensor = torch.tensor(attn_mask).unsqueeze(dim=0)\n","    seg_ids_tensor = torch.tensor(seg_ids).unsqueeze(dim=0)\n","    \n","    outputs = bert_model(token_ids_tensor, attention_mask=attn_mask_tensor,\\\n","                         token_type_ids=seg_ids_tensor)\n","    \n","    #print(outputs.last_hidden_state.shape)\n","    \n","    return None"]},{"cell_type":"code","execution_count":null,"id":"c07e4e34","metadata":{"id":"c07e4e34"},"outputs":[],"source":["for event in raw_PHEME:\n","    for post in event:\n","        text = post['text']\n","        bert_module(text)"]},{"cell_type":"code","execution_count":null,"id":"6d6b21f3","metadata":{"id":"6d6b21f3"},"outputs":[],"source":["def build_tree(event):\n","    # for return \n","    sources = []\n","    destinations = []\n","    timestamps = []\n","    raw_node_features = np.zeros((len(event) + 1, 768))\n","    edge_indexs = np.array([i for i in range(1, len(event))])\n","    #  post ID to index:\n","    id2idx = {}\n","    idx = 1\n","    \n","    source_post = event[0]\n","    id2idx[source_post['id_str']] = idx\n","    raw_node_features[idx] = bert_module(source_post['text'])\n","    idx += 1    \n","    \n","    # id2idx should be down before data preprocessing, in case they are not chronological \n","    for post in event[1:]:\n","        if post['id_str'] not in id2idx:\n","            id2idx[post['id_str']] = idx\n","            idx += 1\n","    # We should differentiate the source post and other reply posts. \n","    for post in event[1:]:\n","        sources.append(id2idx[post['in_reply_to_status_id_str']])\n","        destinations.append(id2idx[post['id_str']])\n","        raw_node_features[id2idx[post['id_str']]] = bert_module(post['text'])\n","        \n","        timestamps.append(post['created_at'] - source_post['created_at'])\n","        \n","    return sources, destinations, timestamps, raw_node_features, edge_indexs\n","        "]},{"cell_type":"code","execution_count":null,"id":"bcbf4651","metadata":{"id":"bcbf4651"},"outputs":[],"source":[]}],"metadata":{"kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.13"},"colab":{"provenance":[],"collapsed_sections":[]}},"nbformat":4,"nbformat_minor":5}